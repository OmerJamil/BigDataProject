{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "metadata": {
        "collapsed": true,
        "id": "L1z6_Ns-xnXp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYSPARK_PYTHON\"] = \"python3\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "8wAI2SVgxnXt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"moive analysis\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "G9SUwlw5xnXv",
        "outputId": "02131dd4-35b9-4ae8-cad9-4e625c827da9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-fe8c45ee39fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"moive analysis\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.some.config.option\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"some-value\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "movies = spark.read.load(\"/FileStore/tables/movies.csv\", format='csv', header = True)\n",
        "ratings = spark.read.load(\"/FileStore/tables/ratings.csv\", format='csv', header = True)\n",
        "links = spark.read.load(\"/FileStore/tables/links.csv\", format='csv', header = True)\n",
        "tags = spark.read.load(\"/FileStore/tables/tags.csv\", format='csv', header = True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-kKSawmRxnXw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "movies.show(5)"
      ],
      "metadata": {
        "id": "0wSsqIh8xnXx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ratings.show(5)"
      ],
      "metadata": {
        "id": "v_QFHi5bxnXz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print('Distinct values of ratings:')\n",
        "print (sorted(ratings.select('rating').distinct().rdd.map(lambda r: r[0]).collect()))"
      ],
      "metadata": {
        "id": "aal6l4VlxnX0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "q1_result = ratings.groupBy('userID').count().orderBy('count', ascending=True)\n",
        "tmp1 = q1_result.select('count').collect()[0]['count']\n",
        "q2_result = ratings.groupBy('movieId').count().orderBy('count', ascending=True)\n",
        "tmp2 = q2_result.select('count').collect()[0]['count']\n",
        "print('For the users that rated movies and the movies that were rated:')\n",
        "print('Minimum number of ratings per user is {}'.format(tmp1))\n",
        "print('Minimum number of ratings per movie is {}'.format(tmp2))"
      ],
      "metadata": {
        "id": "JJTOoA5RxnX2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "count_result = q2_result.filter(q2_result['count'] == 1).count()\n",
        "distinct_movieID = ratings.select('movieId').distinct().count()\n",
        "print('{} out of {} movies are rated by only one user'.format(count_result, distinct_movieID))"
      ],
      "metadata": {
        "id": "_tPTYsesxnX3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Q_LDzxYHxnX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "usersNum = ratings.select(\"userID\").distinct().count()\n",
        "print('Number of users: ', usersNum)"
      ],
      "metadata": {
        "id": "96mqypi4xnX7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "moviesNum = movies.select(\"movieId\").distinct().count()\n",
        "print('Number of movies: ', moviesNum)"
      ],
      "metadata": {
        "id": "DxP2MLfExnX9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "moviesRated = ratings.select(\"movieId\").distinct().collect()\n",
        "moviesRatedNum = ratings.select(\"movieId\").distinct().count()\n",
        "moviesTotal = movies.select(\"movieId\").distinct().collect()\n",
        "print('Number of movies rated by users: ', moviesRatedNum)\n",
        "print('Number of movies not rated before: ', (moviesNum - moviesRatedNum))\n",
        "print('Movies not rated before are: ')\n",
        "for movie in moviesTotal:\n",
        "  if movie not in moviesRated:\n",
        "    print(movie)"
      ],
      "metadata": {
        "id": "zUr99OB0xnX-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from collections import OrderedDict\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.functions import explode\n",
        "tmp = movies.select(\"movieId\",\"title\",\"genres\")\n",
        "split_col = f.split(tmp['genres'], '\\\\|')\n",
        "tmp = tmp.withColumn('split_col', split_col)\n",
        "genresList = tmp.withColumn(\"split_col\", explode(tmp.split_col)).select(\"split_col\").distinct().collect()\n",
        "genresList=[r['split_col'] for r in genresList]\n",
        "genresList"
      ],
      "metadata": {
        "id": "pGFcV2i5xnYA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import array_contains\n",
        "\n",
        "for genre in genresList:\n",
        "  df1 = tmp.filter(array_contains(tmp.split_col, genre))\n",
        "  print (genre + ':')\n",
        "  df1.show()"
      ],
      "metadata": {
        "id": "k8ZcEByyxnYB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator"
      ],
      "metadata": {
        "collapsed": true,
        "id": "M6-vihJ2xnYC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "movie_rating = sc.textFile(\"/FileStore/tables/ratings.csv\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KxbQmctyxnYD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "header = movie_rating.take(1)[0]\n",
        "rating_data = movie_rating.filter(lambda line: line!=header).map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1],tokens[2])).cache()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1zU_WSrDxnYD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rating_data.take(3)"
      ],
      "metadata": {
        "id": "ajaTynxwxnYE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train, validation, test = rating_data.randomSplit([6,2,2],seed = 7856)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pghE2NGgxnYE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train.cache()"
      ],
      "metadata": {
        "id": "Mv2raC1qxnYF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "validation.cache()"
      ],
      "metadata": {
        "id": "UBEAFIccxnYF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test.cache()"
      ],
      "metadata": {
        "id": "Jv0XZaBbxnYG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "def train_ALS(train_data, validation_data, num_iters, reg_param, ranks):\n",
        "    validation_for_predict_RDD = validation_data.map(lambda x: (x[0], x[1]))\n",
        "    min_error = float('inf')\n",
        "    best_rank = -1\n",
        "    best_regularization = 0\n",
        "    best_model = None\n",
        "    for rank, reg, num in itertools.product(ranks, reg_param, num_iters):\n",
        "\n",
        "      model = ALS.train(train_data, iterations=num, rank= rank, lambda_=reg) \n",
        "      \n",
        "      predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
        "      rates_and_preds = validation_data.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
        "      error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
        "      print ('{} latent factors, {} iterations and regularization = {}: validation RMSE is {}'.format(rank, num, reg, error))\n",
        "      if error < min_error:\n",
        "          min_error = error\n",
        "          best_rank = rank\n",
        "          best_num = num\n",
        "          best_regularization = reg\n",
        "          best_model = model\n",
        "    print ('\\nThe best model has {} latent factors, {} iterations and regularization = {}'.format(best_rank, best_num, best_regularization))\n",
        "    return best_model"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bzdI77J1xnYH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "num_iterations = [10,15,20]\n",
        "ranks = [6, 8, 10, 12, 14]\n",
        "reg_params = [0.05, 0.1, 0.2, 0.4, 0.8]\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "final_model = train_ALS(train, validation, num_iterations, reg_params, ranks)\n",
        "\n",
        "print ('Total Runtime: {:.2f} seconds'.format(time.time() - start_time))"
      ],
      "metadata": {
        "scrolled": true,
        "id": "fYIvI8EFxnYH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_learning_curve(iter_array, train_data, validation_data, reg, rank):\n",
        "  validation_for_predict_RDD = validation_data.map(lambda x: (x[0], x[1]))\n",
        "  errors = []\n",
        "  for num_iters in iter_array:\n",
        "    model = ALS.train(train_data, iterations=num_iters, rank= rank, lambda_=reg) \n",
        "    predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
        "    rates_and_preds = validation_data.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
        "    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
        "    errors.append(error)\n",
        "  fig = plt.figure()\n",
        "  ax = plt.axes()\n",
        "  ax.plot(iter_array, errors);\n",
        "  display(fig)"
      ],
      "metadata": {
        "id": "9xkzpxKSxnYI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "iter_array = [1, 2, 5, 10]\n",
        "plot_learning_curve(iter_array, train, validation, 0.2, 10)\n",
        "\n"
      ],
      "metadata": {
        "id": "q38xEpuTxnYJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_for_predict = test.map(lambda x: (x[0], x[1]))\n",
        "\n",
        "predictions = final_model.predictAll(test_for_predict).map(lambda r: ((r[0], r[1]), r[2]))\n",
        "rates_and_preds = test.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
        "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
        "    \n",
        "print ('For testing data the RMSE is %s' % (error))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "n8yib8M2xnYK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext(appName = \"Text Cleaning\")\n",
        "strc = StreamingContext(sc, 3)\n"
      ],
      "metadata": {
        "id": "DeHeMrADLVgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = strc.socketTextStream(\"localhost\", 8084)"
      ],
      "metadata": {
        "id": "3lCaRK2KLakF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_text(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(\"s+\",\" \", sentence)\n",
        "    sentence = re.sub(\"W\",\" \", sentence)\n",
        "    sentence = re.sub(r\"httpS+\", \"\", sentence)\n",
        "    sentence = ' '.join(word for word in sentence.split() if word not in stop_words)\n",
        "    sentence = [lemmatizer.lemmatize(token, \"v\") for token in sentence.split()]\n",
        "    sentence = \" \".join(sentence)\n",
        "    return sentence.strip()"
      ],
      "metadata": {
        "id": "Hf57Jbd2Lf1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strc.start()\n",
        "strc.awaitTermination()"
      ],
      "metadata": {
        "id": "LFVp2bVgLlpg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython2",
      "codemirror_mode": {
        "name": "ipython",
        "version": "2"
      },
      "version": "2.7.14",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "name": "Spark_HW3",
    "notebookId": 2467269044800848,
    "colab": {
      "name": "FinalProjectBigdata.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}